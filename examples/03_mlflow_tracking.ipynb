{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLflow Integration for Audit Tracking\n",
        "\n",
        "This notebook demonstrates how to track Conformal-Drift audits with MLflow for experiment management and reproducibility.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/debu-sinha/conformaldrift/blob/main/examples/03_mlflow_tracking.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install conformal-drift mlflow matplotlib -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import mlflow\n",
        "import matplotlib.pyplot as plt\n",
        "from conformal_drift import ConformalDriftAuditor\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Set Up MLflow Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure MLflow\n",
        "mlflow.set_experiment(\"conformal-drift-audit\")\n",
        "\n",
        "print(f\"Experiment: conformal-drift-audit\")\n",
        "print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Generate Synthetic Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate calibration data\n",
        "n_calibration = 500\n",
        "calibration_scores = np.random.beta(2, 5, n_calibration)\n",
        "\n",
        "# Generate test data\n",
        "n_test = 200\n",
        "test_scores = np.random.beta(2, 5, n_test)\n",
        "test_labels = np.random.binomial(1, 0.9, n_test)\n",
        "\n",
        "test_data = {\n",
        "    'scores': test_scores,\n",
        "    'labels': test_labels\n",
        "}\n",
        "\n",
        "print(f\"Generated {n_calibration} calibration and {n_test} test samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Run Audit with MLflow Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "alpha = 0.1\n",
        "shift_types = ['temporal', 'semantic', 'lexical']\n",
        "shift_intensities = np.linspace(0, 1, 11)\n",
        "\n",
        "with mlflow.start_run(run_name=\"comprehensive_audit\"):\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"alpha\", alpha)\n",
        "    mlflow.log_param(\"n_calibration\", n_calibration)\n",
        "    mlflow.log_param(\"n_test\", n_test)\n",
        "    mlflow.log_param(\"shift_types\", shift_types)\n",
        "    mlflow.log_param(\"n_shift_levels\", len(shift_intensities))\n",
        "    \n",
        "    # Initialize auditor\n",
        "    auditor = ConformalDriftAuditor(\n",
        "        calibration_scores=calibration_scores,\n",
        "        alpha=alpha\n",
        "    )\n",
        "    \n",
        "    all_results = {}\n",
        "    \n",
        "    # Run audits for each shift type\n",
        "    for shift_type in shift_types:\n",
        "        print(f\"\\nRunning {shift_type} shift audit...\")\n",
        "        \n",
        "        with mlflow.start_run(run_name=f\"{shift_type}_shift\", nested=True):\n",
        "            mlflow.log_param(\"shift_type\", shift_type)\n",
        "            \n",
        "            # Run audit\n",
        "            results = auditor.audit(\n",
        "                test_data=test_data,\n",
        "                shift_type=shift_type,\n",
        "                shift_intensity=shift_intensities\n",
        "            )\n",
        "            all_results[shift_type] = results\n",
        "            \n",
        "            # Log coverage at each shift level\n",
        "            for intensity, coverage in zip(results.shift_intensities, results.coverage):\n",
        "                mlflow.log_metric(f\"coverage\", coverage, step=int(intensity * 100))\n",
        "            \n",
        "            # Log summary metrics\n",
        "            mlflow.log_metric(\"max_coverage_gap\", results.max_coverage_gap)\n",
        "            mlflow.log_metric(\"baseline_coverage\", results.coverage[0])\n",
        "            mlflow.log_metric(\"final_coverage\", results.coverage[-1])\n",
        "            \n",
        "            # Create and log coverage curve\n",
        "            fig, ax = plt.subplots(figsize=(8, 5))\n",
        "            ax.plot(results.shift_intensities, results.coverage, 'b-o', linewidth=2)\n",
        "            ax.axhline(y=0.9, color='r', linestyle='--', label='Nominal')\n",
        "            ax.set_xlabel('Shift Intensity')\n",
        "            ax.set_ylabel('Coverage')\n",
        "            ax.set_title(f'{shift_type.capitalize()} Shift Coverage')\n",
        "            ax.legend()\n",
        "            ax.grid(True, alpha=0.3)\n",
        "            fig.savefig(f'{shift_type}_coverage.png', dpi=150)\n",
        "            mlflow.log_artifact(f'{shift_type}_coverage.png')\n",
        "            plt.close()\n",
        "            \n",
        "            # Tag based on results\n",
        "            if results.max_coverage_gap > 0.1:\n",
        "                mlflow.set_tag(\"status\", \"CRITICAL\")\n",
        "            elif results.max_coverage_gap > 0.05:\n",
        "                mlflow.set_tag(\"status\", \"WARNING\")\n",
        "            else:\n",
        "                mlflow.set_tag(\"status\", \"PASS\")\n",
        "            \n",
        "            print(f\"  Max gap: {results.max_coverage_gap:.3f}\")\n",
        "    \n",
        "    # Create comparison plot in parent run\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    colors = {'temporal': 'blue', 'semantic': 'green', 'lexical': 'orange'}\n",
        "    \n",
        "    for shift_type, results in all_results.items():\n",
        "        ax.plot(results.shift_intensities, results.coverage, '-o',\n",
        "                color=colors[shift_type], linewidth=2, label=shift_type.capitalize())\n",
        "    \n",
        "    ax.axhline(y=0.9, color='r', linestyle='--', linewidth=2, label='Nominal')\n",
        "    ax.set_xlabel('Shift Intensity', fontsize=12)\n",
        "    ax.set_ylabel('Coverage', fontsize=12)\n",
        "    ax.set_title('Coverage Comparison Across Shift Types', fontsize=14)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    fig.savefig('comparison.png', dpi=150)\n",
        "    mlflow.log_artifact('comparison.png')\n",
        "    plt.close()\n",
        "    \n",
        "    # Log summary\n",
        "    summary = {\n",
        "        shift_type: {\n",
        "            \"max_gap\": float(r.max_coverage_gap),\n",
        "            \"baseline\": float(r.coverage[0]),\n",
        "            \"final\": float(r.coverage[-1])\n",
        "        }\n",
        "        for shift_type, r in all_results.items()\n",
        "    }\n",
        "    mlflow.log_dict(summary, \"summary.json\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"AUDIT COMPLETE\")\n",
        "    print(f\"Run ID: {mlflow.active_run().info.run_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. View Results in MLflow UI\n",
        "\n",
        "Start the MLflow UI to view results:\n",
        "\n",
        "```bash\n",
        "mlflow ui\n",
        "```\n",
        "\n",
        "Then navigate to http://localhost:5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query runs programmatically\n",
        "runs = mlflow.search_runs(\n",
        "    experiment_names=[\"conformal-drift-audit\"],\n",
        "    filter_string=\"tags.status = 'PASS'\"\n",
        ")\n",
        "\n",
        "print(f\"Found {len(runs)} passing runs\")\n",
        "if len(runs) > 0:\n",
        "    print(runs[['run_id', 'params.shift_type', 'metrics.max_coverage_gap']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Best Practices\n",
        "\n",
        "1. **Always log parameters**: Alpha, sample sizes, shift types\n",
        "2. **Log metrics at steps**: Use shift intensity as step for tracking curves\n",
        "3. **Save artifacts**: Coverage plots, detailed results JSON\n",
        "4. **Use tags**: Mark runs as PASS/WARNING/CRITICAL\n",
        "5. **Nested runs**: Use for multi-shift audits to organize results"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
