{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Auditing RAG Pipelines with LangChain\n",
        "\n",
        "This notebook demonstrates how to use Conformal-Drift to audit RAG pipelines built with LangChain.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/debu-sinha/conformaldrift/blob/main/examples/02_rag_langchain.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install conformal-drift langchain langchain-openai chromadb -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Set your API key\n",
        "os.environ['OPENAI_API_KEY'] = 'your-api-key-here'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Set Up RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"Python is a high-level programming language known for its readability.\",\n",
        "    \"Machine learning is a subset of artificial intelligence.\",\n",
        "    \"Neural networks are inspired by biological neural networks.\",\n",
        "    \"Deep learning uses multiple layers of neural networks.\",\n",
        "    \"Natural language processing deals with text and speech.\",\n",
        "]\n",
        "\n",
        "# Set up embeddings and vector store\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_texts(documents, embeddings)\n",
        "\n",
        "# Create RAG chain\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        ")\n",
        "\n",
        "print(\"RAG pipeline set up successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Nonconformity Score Function\n",
        "\n",
        "We'll use the similarity between retrieved documents and the response as the basis for nonconformity scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_nonconformity_score(query, response, retrieved_docs):\n",
        "    \"\"\"\n",
        "    Compute nonconformity score based on response-document alignment.\n",
        "    Lower scores = more conforming (response aligns with documents)\n",
        "    Higher scores = less conforming (potential hallucination)\n",
        "    \"\"\"\n",
        "    if not retrieved_docs:\n",
        "        return 1.0  # Maximum nonconformity if no docs retrieved\n",
        "    \n",
        "    # Get embeddings\n",
        "    response_emb = embeddings.embed_query(response)\n",
        "    doc_embs = [embeddings.embed_query(doc) for doc in retrieved_docs]\n",
        "    \n",
        "    # Compute similarities\n",
        "    similarities = []\n",
        "    for doc_emb in doc_embs:\n",
        "        similarity = np.dot(response_emb, doc_emb) / (\n",
        "            np.linalg.norm(response_emb) * np.linalg.norm(doc_emb)\n",
        "        )\n",
        "        similarities.append(similarity)\n",
        "    \n",
        "    # Nonconformity = 1 - max similarity\n",
        "    return 1 - max(similarities)\n",
        "\n",
        "print(\"Nonconformity function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Collect Calibration Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calibration queries\n",
        "calibration_queries = [\n",
        "    \"What is Python?\",\n",
        "    \"What is machine learning?\",\n",
        "    \"How do neural networks work?\",\n",
        "    \"What is deep learning?\",\n",
        "    \"What is NLP?\",\n",
        "    # Add more queries for better calibration...\n",
        "]\n",
        "\n",
        "# Run calibration\n",
        "calibration_scores = []\n",
        "\n",
        "for query in calibration_queries:\n",
        "    # Get response and retrieved docs\n",
        "    result = qa_chain({\"query\": query})\n",
        "    response = result['result']\n",
        "    \n",
        "    # Get retrieved docs\n",
        "    docs = vectorstore.similarity_search(query, k=2)\n",
        "    doc_texts = [doc.page_content for doc in docs]\n",
        "    \n",
        "    # Compute score\n",
        "    score = compute_nonconformity_score(query, response, doc_texts)\n",
        "    calibration_scores.append(score)\n",
        "    print(f\"Query: '{query[:30]}...' -> Score: {score:.3f}\")\n",
        "\n",
        "calibration_scores = np.array(calibration_scores)\n",
        "print(f\"\\nCalibration complete: {len(calibration_scores)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Initialize Auditor and Run Audit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from conformal_drift import ConformalDriftAuditor\n",
        "\n",
        "# Initialize auditor\n",
        "auditor = ConformalDriftAuditor(\n",
        "    calibration_scores=calibration_scores,\n",
        "    alpha=0.1  # 90% coverage target\n",
        ")\n",
        "\n",
        "# Test queries (potentially shifted domain)\n",
        "test_queries = [\n",
        "    \"What is reinforcement learning?\",  # Related but not in corpus\n",
        "    \"How does GPT work?\",  # Different domain\n",
        "    \"What is Python used for?\",  # In domain\n",
        "    # Add more test queries...\n",
        "]\n",
        "\n",
        "# Collect test scores\n",
        "test_scores = []\n",
        "test_labels = []  # 1 if response is grounded, 0 if hallucination\n",
        "\n",
        "for query in test_queries:\n",
        "    result = qa_chain({\"query\": query})\n",
        "    response = result['result']\n",
        "    \n",
        "    docs = vectorstore.similarity_search(query, k=2)\n",
        "    doc_texts = [doc.page_content for doc in docs]\n",
        "    \n",
        "    score = compute_nonconformity_score(query, response, doc_texts)\n",
        "    test_scores.append(score)\n",
        "    \n",
        "    # For demo, manually label (in practice, use ground truth)\n",
        "    test_labels.append(1 if score < 0.5 else 0)\n",
        "\n",
        "print(f\"Test samples: {len(test_scores)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run audit\n",
        "test_data = {\n",
        "    'scores': np.array(test_scores),\n",
        "    'labels': np.array(test_labels)\n",
        "}\n",
        "\n",
        "results = auditor.audit(\n",
        "    test_data=test_data,\n",
        "    shift_type=\"semantic\",\n",
        "    shift_intensity=np.linspace(0, 1, 6)\n",
        ")\n",
        "\n",
        "print(\"\\nAudit Results:\")\n",
        "print(\"-\" * 40)\n",
        "for i, cov in zip(results.shift_intensities, results.coverage):\n",
        "    print(f\"Shift {i:.0%}: Coverage = {cov:.3f}\")\n",
        "print(f\"\\nMax coverage gap: {results.max_coverage_gap:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Recommendations\n",
        "\n",
        "Based on the audit results, you can:\n",
        "\n",
        "1. **Expand knowledge base** if coverage drops due to out-of-domain queries\n",
        "2. **Recalibrate** if coverage degrades significantly under shift\n",
        "3. **Add guardrails** to abstain on high-nonconformity predictions\n",
        "4. **Monitor in production** using continuous auditing"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
