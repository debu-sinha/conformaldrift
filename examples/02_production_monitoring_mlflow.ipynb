{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Production RAG Monitoring with MLflow\n",
        "\n",
        "This notebook shows how to set up production monitoring for your RAG system's hallucination guardrails using MLflow.\n",
        "\n",
        "**Real-world use case:**\n",
        "- Track guardrail performance over time\n",
        "- Detect when your coverage starts degrading\n",
        "- Compare different guardrail configurations\n",
        "- Get alerts when coverage drops below threshold\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/debu-sinha/conformaldrift/blob/main/examples/02_production_monitoring_mlflow.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install conformal-drift mlflow langchain langchain-openai sentence-transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import mlflow\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = 'your-key-here'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up MLflow Experiment\n",
        "\n",
        "Create a dedicated experiment for tracking your RAG guardrails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create experiment for guardrail monitoring\n",
        "mlflow.set_experiment(\"rag-hallucination-guardrails\")\n",
        "\n",
        "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "print(f\"Experiment: rag-hallucination-guardrails\")\n",
        "print(\"\\nRun 'mlflow ui' to view the dashboard\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Your RAG System Components\n",
        "\n",
        "This is a template - replace with your actual RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Your embedding model for computing hallucination scores\n",
        "scorer = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def compute_hallucination_score(response: str, context: str) -> float:\n",
        "    \"\"\"\n",
        "    Compute hallucination score for a RAG response.\n",
        "    Replace this with your actual scoring logic.\n",
        "    \"\"\"\n",
        "    if not context:\n",
        "        return 1.0\n",
        "    \n",
        "    response_emb = scorer.encode([response])\n",
        "    context_emb = scorer.encode([context])\n",
        "    similarity = cosine_similarity(response_emb, context_emb)[0][0]\n",
        "    \n",
        "    return 1 - similarity\n",
        "\n",
        "print(\"Scoring function ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scenario 1: Initial Calibration Run\n",
        "\n",
        "Log your initial calibration as a baseline run in MLflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from conformal_drift import ConformalDriftAuditor\n",
        "\n",
        "# Simulate calibration data (replace with your real calibration data)\n",
        "np.random.seed(42)\n",
        "calibration_responses = [\n",
        "    (\"Python uses indentation for code blocks.\", \"Python syntax uses whitespace indentation.\"),\n",
        "    (\"Lists are created with square brackets.\", \"Create lists using [] syntax.\"),\n",
        "    (\"Dictionaries store key-value pairs.\", \"Dict maps keys to values.\"),\n",
        "    # ... add your real calibration data here\n",
        "]\n",
        "\n",
        "# Compute calibration scores\n",
        "calibration_scores = []\n",
        "for response, context in calibration_responses:\n",
        "    score = compute_hallucination_score(response, context)\n",
        "    calibration_scores.append(score)\n",
        "\n",
        "# For demo, generate more scores\n",
        "calibration_scores = np.random.beta(2, 5, 100)  # Replace with real scores!\n",
        "\n",
        "with mlflow.start_run(run_name=\"initial_calibration\"):\n",
        "    # Log calibration parameters\n",
        "    mlflow.log_param(\"alpha\", 0.1)\n",
        "    mlflow.log_param(\"n_calibration_samples\", len(calibration_scores))\n",
        "    mlflow.log_param(\"embedding_model\", \"all-MiniLM-L6-v2\")\n",
        "    mlflow.log_param(\"calibration_date\", datetime.now().isoformat())\n",
        "    \n",
        "    # Compute threshold\n",
        "    threshold = np.percentile(calibration_scores, 90)\n",
        "    mlflow.log_metric(\"threshold\", threshold)\n",
        "    mlflow.log_metric(\"calibration_mean_score\", np.mean(calibration_scores))\n",
        "    mlflow.log_metric(\"calibration_std_score\", np.std(calibration_scores))\n",
        "    \n",
        "    # Save calibration scores as artifact\n",
        "    np.save(\"calibration_scores.npy\", calibration_scores)\n",
        "    mlflow.log_artifact(\"calibration_scores.npy\")\n",
        "    \n",
        "    # Tag as baseline\n",
        "    mlflow.set_tag(\"type\", \"calibration\")\n",
        "    mlflow.set_tag(\"status\", \"baseline\")\n",
        "    \n",
        "    print(f\"Logged calibration run\")\n",
        "    print(f\"Threshold: {threshold:.4f}\")\n",
        "    print(f\"Run ID: {mlflow.active_run().info.run_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scenario 2: Daily/Weekly Audit Runs\n",
        "\n",
        "Run this periodically to track how your guardrail performs on production traffic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_production_audit(production_data, calibration_scores, audit_name=\"production_audit\"):\n",
        "    \"\"\"\n",
        "    Run a production audit and log to MLflow.\n",
        "    \n",
        "    Args:\n",
        "        production_data: List of (response, context, is_grounded) tuples\n",
        "        calibration_scores: Original calibration scores\n",
        "        audit_name: Name for the MLflow run\n",
        "    \"\"\"\n",
        "    with mlflow.start_run(run_name=audit_name):\n",
        "        # Log metadata\n",
        "        mlflow.log_param(\"audit_date\", datetime.now().isoformat())\n",
        "        mlflow.log_param(\"n_samples\", len(production_data))\n",
        "        mlflow.set_tag(\"type\", \"audit\")\n",
        "        \n",
        "        # Compute scores on production data\n",
        "        production_scores = []\n",
        "        labels = []\n",
        "        \n",
        "        for response, context, is_grounded in production_data:\n",
        "            score = compute_hallucination_score(response, context)\n",
        "            production_scores.append(score)\n",
        "            labels.append(is_grounded)\n",
        "        \n",
        "        production_scores = np.array(production_scores)\n",
        "        labels = np.array(labels)\n",
        "        \n",
        "        # Get threshold from calibration\n",
        "        threshold = np.percentile(calibration_scores, 90)\n",
        "        \n",
        "        # Compute coverage on grounded responses\n",
        "        grounded_scores = production_scores[labels]\n",
        "        if len(grounded_scores) > 0:\n",
        "            coverage = np.mean(grounded_scores <= threshold)\n",
        "        else:\n",
        "            coverage = np.nan\n",
        "        \n",
        "        # Compute precision on flagged responses\n",
        "        flagged = production_scores > threshold\n",
        "        if sum(flagged) > 0:\n",
        "            precision = np.mean(~labels[flagged])  # True positive rate\n",
        "        else:\n",
        "            precision = np.nan\n",
        "        \n",
        "        # Log metrics\n",
        "        mlflow.log_metric(\"coverage\", coverage)\n",
        "        mlflow.log_metric(\"coverage_gap\", 0.9 - coverage)\n",
        "        mlflow.log_metric(\"precision\", precision if not np.isnan(precision) else 0)\n",
        "        mlflow.log_metric(\"flagged_rate\", np.mean(flagged))\n",
        "        mlflow.log_metric(\"mean_score\", np.mean(production_scores))\n",
        "        mlflow.log_metric(\"score_shift\", np.mean(production_scores) - np.mean(calibration_scores))\n",
        "        \n",
        "        # Detect distribution shift\n",
        "        score_shift = np.mean(production_scores) - np.mean(calibration_scores)\n",
        "        \n",
        "        # Set status based on coverage\n",
        "        if coverage >= 0.85:\n",
        "            mlflow.set_tag(\"status\", \"HEALTHY\")\n",
        "        elif coverage >= 0.75:\n",
        "            mlflow.set_tag(\"status\", \"WARNING\")\n",
        "        else:\n",
        "            mlflow.set_tag(\"status\", \"CRITICAL\")\n",
        "        \n",
        "        # Create visualization\n",
        "        import matplotlib.pyplot as plt\n",
        "        \n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        \n",
        "        # Score distribution comparison\n",
        "        axes[0].hist(calibration_scores, bins=20, alpha=0.5, label='Calibration', density=True)\n",
        "        axes[0].hist(production_scores, bins=20, alpha=0.5, label='Production', density=True)\n",
        "        axes[0].axvline(threshold, color='r', linestyle='--', label=f'Threshold ({threshold:.3f})')\n",
        "        axes[0].set_xlabel('Hallucination Score')\n",
        "        axes[0].set_ylabel('Density')\n",
        "        axes[0].set_title('Score Distribution Shift')\n",
        "        axes[0].legend()\n",
        "        \n",
        "        # Coverage gauge\n",
        "        axes[1].bar(['Coverage'], [coverage], color='green' if coverage >= 0.85 else 'orange' if coverage >= 0.75 else 'red')\n",
        "        axes[1].axhline(0.9, color='r', linestyle='--', label='Target (90%)')\n",
        "        axes[1].set_ylim(0, 1)\n",
        "        axes[1].set_ylabel('Coverage')\n",
        "        axes[1].set_title(f'Coverage: {coverage:.1%}')\n",
        "        axes[1].legend()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('audit_report.png', dpi=150)\n",
        "        mlflow.log_artifact('audit_report.png')\n",
        "        plt.close()\n",
        "        \n",
        "        print(f\"Audit complete: Coverage = {coverage:.1%}\")\n",
        "        return coverage\n",
        "\n",
        "print(\"Audit function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate production data with some shift\n",
        "# In practice, replace this with your actual production data\n",
        "\n",
        "def simulate_production_data(shift_level=0.0):\n",
        "    \"\"\"Simulate production data with distribution shift.\"\"\"\n",
        "    np.random.seed(int(datetime.now().timestamp()) % 1000)\n",
        "    \n",
        "    n_samples = 50\n",
        "    production_data = []\n",
        "    \n",
        "    for _ in range(n_samples):\n",
        "        # Simulate grounded vs hallucinated responses\n",
        "        is_grounded = np.random.random() > 0.2  # 80% grounded\n",
        "        \n",
        "        if is_grounded:\n",
        "            # Grounded response - score should be low\n",
        "            # But with shift, scores increase\n",
        "            base_score = np.random.beta(2, 5)\n",
        "            score = base_score + shift_level * 0.3  # Shift increases scores\n",
        "        else:\n",
        "            # Hallucination - score should be high\n",
        "            score = np.random.beta(5, 2)\n",
        "        \n",
        "        production_data.append((f\"response_{_}\", f\"context_{_}\", is_grounded))\n",
        "    \n",
        "    return production_data\n",
        "\n",
        "# Run audit for \"Week 1\" - no shift\n",
        "prod_data_week1 = simulate_production_data(shift_level=0.0)\n",
        "run_production_audit(prod_data_week1, calibration_scores, \"week_1_audit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run audit for \"Week 2\" - mild shift\n",
        "prod_data_week2 = simulate_production_data(shift_level=0.3)\n",
        "run_production_audit(prod_data_week2, calibration_scores, \"week_2_audit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run audit for \"Week 3\" - significant shift (e.g., new product launch)\n",
        "prod_data_week3 = simulate_production_data(shift_level=0.6)\n",
        "run_production_audit(prod_data_week3, calibration_scores, \"week_3_audit\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scenario 3: Compare Guardrail Configurations\n",
        "\n",
        "A/B test different embedding models or thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "configurations = [\n",
        "    {\"name\": \"minilm-v2\", \"model\": \"all-MiniLM-L6-v2\", \"alpha\": 0.1},\n",
        "    {\"name\": \"minilm-conservative\", \"model\": \"all-MiniLM-L6-v2\", \"alpha\": 0.05},  # More conservative\n",
        "    {\"name\": \"minilm-relaxed\", \"model\": \"all-MiniLM-L6-v2\", \"alpha\": 0.2},  # More relaxed\n",
        "]\n",
        "\n",
        "with mlflow.start_run(run_name=\"config_comparison\"):\n",
        "    mlflow.set_tag(\"type\", \"comparison\")\n",
        "    \n",
        "    for config in configurations:\n",
        "        with mlflow.start_run(run_name=config[\"name\"], nested=True):\n",
        "            mlflow.log_param(\"embedding_model\", config[\"model\"])\n",
        "            mlflow.log_param(\"alpha\", config[\"alpha\"])\n",
        "            \n",
        "            # Compute threshold for this config\n",
        "            threshold = np.percentile(calibration_scores, (1 - config[\"alpha\"]) * 100)\n",
        "            mlflow.log_metric(\"threshold\", threshold)\n",
        "            \n",
        "            # Simulate evaluation (replace with real evaluation)\n",
        "            coverage = 1 - config[\"alpha\"] - np.random.uniform(0, 0.1)\n",
        "            false_positive_rate = config[\"alpha\"] + np.random.uniform(0, 0.05)\n",
        "            \n",
        "            mlflow.log_metric(\"coverage\", coverage)\n",
        "            mlflow.log_metric(\"false_positive_rate\", false_positive_rate)\n",
        "            \n",
        "            print(f\"{config['name']}: Coverage={coverage:.1%}, FPR={false_positive_rate:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Query MLflow for Insights\n",
        "\n",
        "Find runs where coverage dropped below threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find all audit runs with coverage issues\n",
        "warning_runs = mlflow.search_runs(\n",
        "    experiment_names=[\"rag-hallucination-guardrails\"],\n",
        "    filter_string=\"tags.type = 'audit' AND tags.status != 'HEALTHY'\",\n",
        "    order_by=[\"metrics.coverage ASC\"]\n",
        ")\n",
        "\n",
        "if len(warning_runs) > 0:\n",
        "    print(\"Runs with coverage issues:\")\n",
        "    print(warning_runs[['run_id', 'tags.status', 'metrics.coverage', 'metrics.score_shift']].to_string())\n",
        "else:\n",
        "    print(\"No coverage issues detected!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Track coverage over time\n",
        "all_audits = mlflow.search_runs(\n",
        "    experiment_names=[\"rag-hallucination-guardrails\"],\n",
        "    filter_string=\"tags.type = 'audit'\",\n",
        "    order_by=[\"start_time ASC\"]\n",
        ")\n",
        "\n",
        "if len(all_audits) > 0:\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    \n",
        "    ax.plot(range(len(all_audits)), all_audits['metrics.coverage'], 'b-o', linewidth=2)\n",
        "    ax.axhline(0.9, color='r', linestyle='--', label='Target')\n",
        "    ax.axhline(0.85, color='orange', linestyle='--', label='Warning')\n",
        "    \n",
        "    ax.set_xlabel('Audit Run')\n",
        "    ax.set_ylabel('Coverage')\n",
        "    ax.set_title('Guardrail Coverage Over Time')\n",
        "    ax.set_xticks(range(len(all_audits)))\n",
        "    ax.set_xticklabels([r.split('_')[-2] + '_' + r.split('_')[-1] for r in all_audits['tags.mlflow.runName']], rotation=45)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('coverage_trend.png', dpi=150)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Production Integration Template\n",
        "\n",
        "Here's how to integrate this into your production system:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Production monitoring class\n",
        "class RAGGuardrailMonitor:\n",
        "    def __init__(self, calibration_scores, alpha=0.1, experiment_name=\"rag-guardrails\"):\n",
        "        self.calibration_scores = calibration_scores\n",
        "        self.threshold = np.percentile(calibration_scores, (1 - alpha) * 100)\n",
        "        self.alpha = alpha\n",
        "        self.experiment_name = experiment_name\n",
        "        self.scores_buffer = []\n",
        "        self.labels_buffer = []\n",
        "        \n",
        "        mlflow.set_experiment(experiment_name)\n",
        "    \n",
        "    def log_prediction(self, hallucination_score: float, is_grounded: bool):\n",
        "        \"\"\"Log a single prediction for batch analysis.\"\"\"\n",
        "        self.scores_buffer.append(hallucination_score)\n",
        "        self.labels_buffer.append(is_grounded)\n",
        "    \n",
        "    def run_audit(self, run_name=None):\n",
        "        \"\"\"Run audit on buffered predictions.\"\"\"\n",
        "        if len(self.scores_buffer) < 10:\n",
        "            print(\"Not enough samples for audit\")\n",
        "            return\n",
        "        \n",
        "        if run_name is None:\n",
        "            run_name = f\"audit_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
        "        \n",
        "        with mlflow.start_run(run_name=run_name):\n",
        "            scores = np.array(self.scores_buffer)\n",
        "            labels = np.array(self.labels_buffer)\n",
        "            \n",
        "            grounded_scores = scores[labels]\n",
        "            coverage = np.mean(grounded_scores <= self.threshold) if len(grounded_scores) > 0 else np.nan\n",
        "            \n",
        "            mlflow.log_metric(\"coverage\", coverage)\n",
        "            mlflow.log_metric(\"n_samples\", len(scores))\n",
        "            mlflow.log_metric(\"mean_score\", np.mean(scores))\n",
        "            \n",
        "            if coverage < 0.85:\n",
        "                mlflow.set_tag(\"alert\", \"COVERAGE_DROP\")\n",
        "            \n",
        "            # Clear buffer\n",
        "            self.scores_buffer = []\n",
        "            self.labels_buffer = []\n",
        "            \n",
        "            return coverage\n",
        "\n",
        "# Usage:\n",
        "# monitor = RAGGuardrailMonitor(calibration_scores)\n",
        "# \n",
        "# # In your RAG pipeline:\n",
        "# score = compute_hallucination_score(response, context)\n",
        "# is_hallucination = score > monitor.threshold\n",
        "# monitor.log_prediction(score, is_grounded=not is_hallucination)\n",
        "# \n",
        "# # Run audit daily/weekly:\n",
        "# coverage = monitor.run_audit()\n",
        "\n",
        "print(\"RAGGuardrailMonitor class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View Results\n",
        "\n",
        "Run this in your terminal to view the MLflow dashboard:\n",
        "\n",
        "```bash\n",
        "mlflow ui\n",
        "```\n",
        "\n",
        "Then open http://localhost:5000"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
